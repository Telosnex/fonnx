import 'dart:io';

import 'package:flutter/foundation.dart';
import 'package:flutter_test/flutter_test.dart';
import 'package:fonnx/models/minilml6v2/mini_lm_l6_v2.dart';

void main() {
  const tokenizer = MiniLmL6V2.tokenizer;

  group('tokenize', () {
    test('hello world tokens', () async {
      final tokens = tokenizer.tokenize('hello world');
      expect(tokens.first.tokens, equals([101, 7592, 2088, 102]));
    });

    test('emoji', () async {
      final result = tokenizer.tokenize('üëç');
      expect(result.first.tokens, equals([101, 100, 102]));
    });

    test('text & emoji', () async {
      final result = tokenizer.tokenize('That looks great üëç');
      expect(result.first.tokens, equals([101, 2008, 3504, 2307, 100, 102]));
    });

    test('reverses to original', () async {
      final input = "Hello, world!";
      final result = tokenizer.tokenize(input);
      expect(result.first.text, equals(input));
    });

    test('chinese', () async {
      // String from https://github.com/huggingface/tokenizers/blob/0d8c57da48319a91fe9cd3e31a36b9bd29a8292c/tokenizers/src/pre_tokenizers/bert.rs#L52
      final result = tokenizer.tokenize('ÈáéÂè£Èáå‰Ω≥ Noguchi Rika');
      expect(
          result.first.tokens,
          equals(
              [101, 1963, 30314, 30488, 100, 2053, 16918, 15544, 2912, 102]));
    });

    test('extremely long text stays within model limit', () async {
      const lipsum = '''
Lorem ipsum dolor sit amet, id assum putant est, in vis aliquid molestiae. Diam scriptorem delicatissimi nec no, usu an possim sensibus, maluisset consectetuer vis te. Ex suas lobortis mei. Ei scripta copiosae duo, ridens definitionem duo eu, prima oportere in ius.

Te has impedit oporteat, per te eruditi consetetur, magna libris blandit ea per. Nec euripidis intellegat accommodare ad. Sea sint autem doctus an. Ipsum aliquip perpetua vim te. Mel facilisi quaestio te, vix in ignota impedit delicata, et impetus similique pri.

Sententiae adipiscing vim ei. Cum at accusamus similique. Eum no nulla labitur, id mei eros aliquip, sed cu putent maluisset. Vel et congue partem convenire.

Aliquam feugiat vel ex, vim et simul perfecto singulis, pri ad deseruisse adipiscing. Mea elitr rationibus ex, ad sadipscing persequeris eloquentiam eos. Expetendis quaerendum reformidans ad pri, cibo dissentias pro ne. Vis facer offendit pertinacia et, eu sit rebum appareat, te patrioque evertitur cum. Timeam prompta nam no. Ius ex atqui repudiare, justo mediocrem cum ad, nec utinam erroribus reformidans et.

Qui rebum delectus et, ad elit deserunt inimicus quo, vix ne molestie dissentias suscipiantur. Ei has oratio veniam nostro, pri at laudem impedit consulatu. Semper denique te usu, quando epicurei nam cu, ut eleifend temporibus sit. Impetus laoreet mentitum quo in.''';
      final result = tokenizer.tokenize(lipsum);
      expect(result.length, 2);
      final stringOne = tokenizer.detokenize(result.first.tokens);
      final stringTwo = tokenizer.detokenize(result.last.tokens);
      expect('$stringOne $stringTwo',
          equals(lipsum.toLowerCase().replaceAll('\n\n', ' ')));
      // This ensures the input string did exceed the model's max input length.
    });

    test('speed test', skip: true, () async {
      final sw = Stopwatch()..start();
      const trials = 200;
      for (var i = 0; i < trials; i++) {
        const lipsum = '''
Lorem ipsum dolor sit amet, id assum putant est, in vis aliquid molestiae. Diam scriptorem delicatissimi nec no, usu an possim sensibus, maluisset consectetuer vis te. Ex suas lobortis mei. Ei scripta copiosae duo, ridens definitionem duo eu, prima oportere in ius.

Te has impedit oporteat, per te eruditi consetetur, magna libris blandit ea per. Nec euripidis intellegat accommodare ad. Sea sint autem doctus an. Ipsum aliquip perpetua vim te. Mel facilisi quaestio te, vix in ignota impedit delicata, et impetus similique pri.

Sententiae adipiscing vim ei. Cum at accusamus similique. Eum no nulla labitur, id mei eros aliquip, sed cu putent maluisset. Vel et congue partem convenire.

Aliquam feugiat vel ex, vim et simul perfecto singulis, pri ad deseruisse adipiscing. Mea elitr rationibus ex, ad sadipscing persequeris eloquentiam eos. Expetendis quaerendum reformidans ad pri, cibo dissentias pro ne. Vis facer offendit pertinacia et, eu sit rebum appareat, te patrioque evertitur cum. Timeam prompta nam no. Ius ex atqui repudiare, justo mediocrem cum ad, nec utinam erroribus reformidans et.

Qui rebum delectus et, ad elit deserunt inimicus quo, vix ne molestie dissentias suscipiantur. Ei has oratio veniam nostro, pri at laudem impedit consulatu. Semper denique te usu, quando epicurei nam cu, ut eleifend temporibus sit. Impetus laoreet mentitum quo in.''';
        final _ = tokenizer.tokenize(lipsum);
      }
      sw.stop();
      debugPrint(
          'Speed test: ${sw.elapsedMilliseconds / trials} ms. Total: ${sw.elapsedMilliseconds} ms');
    });

    test('sentence', () async {
      final result = tokenizer.tokenize(
          'the dogs caused much consternation the rain in spain falls mainly on the plain');
      expect(
          result.first.tokens,
          equals([
            101,
            1996,
            6077,
            3303,
            2172,
            9530,
            6238,
            9323,
            1996,
            4542,
            1999,
            3577,
            4212,
            3701,
            2006,
            1996,
            5810,
            102
          ]));
    });
  });

  group('languages', () {
    test('Arabic', () {
      const string = "ÿßŸÑÿ≠Ÿäÿßÿ© ÿ¨ŸÖŸäŸÑÿ©";
      final result = tokenizer.tokenize(string);
      expect(
          result.first.tokens,
          equals([
            101,
            1270,
            23673,
            29820,
            14498,
            25573,
            19433,
            1275,
            22192,
            14498,
            23673,
            19433,
            102
          ]));
      final detokenized = tokenizer.detokenize(result.first.tokens);
      expect(detokenized, equals("ÿßŸÑÿ≠Ÿäÿßÿ© ÿ¨ŸÖŸäŸÑÿ©"));
    });

    test('French', () {
      const string = "L'amour est l'ingr√©dient principal de la vie.";
      final result = tokenizer.tokenize(string);
      expect(
          result.first.tokens,
          equals([
            101,
            1048,
            29618,
            22591,
            3126,
            9765,
            1048,
            29618,
            2075,
            5596,
            11638,
            4054,
            2139,
            2474,
            20098,
            29625,
            102
          ]));
      final detokenized = tokenizer.detokenize(result.first.tokens);
      expect(
          detokenized, equals("l'amour est l'ingredient principal de la vie."));
    });

    test('German', () {
      const string = "Das Leben ist wundersch√∂n, wenn du es liebst.";
      final result = tokenizer.tokenize(string);
      expect(
          result.first.tokens,
          equals([
            101,
            8695,
            3393,
            10609,
            21541,
            8814,
            11563,
            11624,
            2239,
            29623,
            19181,
            2078,
            4241,
            9686,
            4682,
            5910,
            2102,
            29625,
            102
          ]));
      final detokenized = tokenizer.detokenize(result.first.tokens);
      expect(
          detokenized, equals('das leben ist wunderschon, wenn du es liebst.'));
    });

    test('Japanese', () {
      const string = "„Ç¢„É°„É™„Ç´‰∫∫„Åß„Åô„ÄÇ";
      final result = tokenizer.tokenize(string);
      expect(
          result.first.tokens,
          equals(
              [101, 1693, 30252, 30258, 30226, 30282, 100, 30184, 30162, 102]));
      final detokenized = tokenizer.detokenize(result.first.tokens);
      expect(detokenized, equals('„Ç¢„É°„É™„Ç´‰∫∫ [UNK]„Åô„ÄÇ'));
    });

    test('Portuguese', () {
      const string = "O c√©u est√° cheio de estrelas invis√≠veis.";
      final result = tokenizer.tokenize(string);
      expect(
          result.first.tokens,
          equals([
            101,
            1051,
            8292,
            2226,
            9765,
            2050,
            18178,
            3695,
            2139,
            9765,
            16570,
            3022,
            1999,
            11365,
            3512,
            2483,
            29625,
            102
          ]));
      final detokenized = tokenizer.detokenize(result.first.tokens);
      expect(detokenized, equals('o ceu esta cheio de estrelas invisiveis.'));
    });

    test('Spanish', () {
      const string = 'La felicidad est√° hecha de peque√±os momentos.';
      final result = tokenizer.tokenize(string);
      expect(
          result.first.tokens,
          equals([
            101,
            2474,
            10768,
            10415,
            27893,
            9765,
            2050,
            2002,
            7507,
            2139,
            21877,
            4226,
            15460,
            2617,
            2891,
            29625,
            102
          ]));
      final detokenized = tokenizer.detokenize(result.first.tokens);
      expect(
          detokenized, equals('la felicidad esta hecha de pequenos momentos.'));
    });

    test('Russian', skip: 'odd behavior on CI, see below', () {
      // CI reports failure "at location [13] is <29436> instead of <2078>"
      // This passes locally on a MacBook M2, yet, fails on CI.
      const string = "–ü—É—Ç–∏–Ω —Å–ø–æ—Ç–∫–Ω—É–ª—Å—è.";
      final result = tokenizer.tokenize(string);
      expect(
          result.first.tokens,
          equals([
            101,
            1194,
            29748,
            22919,
            10325,
            18947,
            1196,
            29746,
            14150,
            22919,
            23925,
            18947,
            29748,
            2078,
            29747,
            17432,
            29625,
            102
          ]));
      final detokenized = tokenizer.detokenize(result.first.tokens);
      // Note 3rd to last letter changes: this only happened after stripping
      // accents in order to support ex. French/German/Spanish examples.
      // Before that, it was exactly equal. This is worth following up on,
      // but isn't critical currently.
      expect(detokenized, equals('–ø—É—Ç–∏–Ω —Å–ø–æ—Ç–∫–Ω—Én—Å—è.'));
    });
  });
  group('detokenize', () {
    test('hello world', () {
      final string = tokenizer.detokenize([101, 7592, 2088, 102]);
      expect(string, equals('hello world'));
    });

    test('emoji', () async {
      final string = tokenizer.detokenize([101, 100, 102]);
      expect(string, equals('[UNK]'));
    });

    test('text & emoji', () async {
      final string = tokenizer.detokenize([101, 2008, 3504, 2307, 100, 102]);
      expect(string, equals('that looks great [UNK]'));
    });

    test('chinese', () async {
      final tokens = tokenizer.detokenize(
          [101, 1963, 30314, 30488, 100, 2053, 16918, 15544, 2912, 102]);
      expect(tokens, equals('ÈáéÂè£Èáå [UNK] noguchi rika'));
    });

    test('sentence', () async {
      final string = tokenizer.detokenize([
        101,
        1996,
        6077,
        3303,
        2172,
        9530,
        6238,
        9323,
        1996,
        4542,
        1999,
        3577,
        4212,
        3701,
        2006,
        1996,
        5810,
        102
      ]);
      expect(
          string,
          equals(
              'the dogs caused much consternation the rain in spain falls mainly on the plain'));
    });
  });

  test('Generate coders', skip: 'coder generator', () async {
    String vocabPath = 'test/tokenizers/bert-base-uncased-vocab.txt';
    File file = File(vocabPath);
    final string = await file.readAsString();
    final strings = <String>[];
    final stringToTokenIndex = <String, int>{};
    for (final line in string.split('\n')) {
      final string = line.trimRight();
      if (string.isEmpty) {
        debugPrint('WARNING: vocab has empty string');
        continue;
      }
      if (stringToTokenIndex.containsKey(string)) {
        debugPrint('WARNING: vocab has seemingly duplicate string `$string`');
        continue;
      }
      strings.add(string);
    }

    final encoder = StringBuffer();
    final decoder = StringBuffer();
    encoder.writeln('const Map<String, int> bertEncoder = {');
    decoder.writeln('const List<String> bertDecoder = [');
    for (final (index, string) in strings.indexed) {
      // Its complicated enough that it can't be auto-fixed and I do not watch
      // to touch it right now. :^)
      // ignore: prefer_interpolation_to_compose_strings
      final separator = string.contains('"') ? "'''" : '"""';
      encoder.writeln("  r$separator$string$separator: $index,");
      decoder.writeln("  r$separator$string$separator,");
    }
    encoder.writeln('};');
    decoder.writeln('];');

    final encoderString = encoder.toString();
    final decoderString = decoder.toString();
    const outputPath = 'test/outputs/bert_vocab.dart';
    final outputFile = File(outputPath);
    await outputFile.writeAsString('$encoderString\n\n$decoderString');
  });
}
