<!DOCTYPE html>
<html>

<head>
  <!--
    If you are serving your web app in a path other than the root, change the
    href value below to reflect the base path you are serving from.

    The path provided below has to start and end with a slash "/" in order for
    it to work correctly.

    For more details:
    * https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base

    This is a placeholder for base href that will be replaced by the value of
    the `--base-href` argument provided to `flutter build`.
  -->
  <base href="$FLUTTER_BASE_HREF">

  <meta charset="UTF-8">
  <meta content="IE=Edge" http-equiv="X-UA-Compatible">
  <meta name="description" content="Demonstrates how to use the fonnx plugin.">

  <!-- iOS meta tags & icons -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="apple-mobile-web-app-title" content="fonnx_example">
  <link rel="apple-touch-icon" href="icons/Icon-192.png">

  <!-- Favicon -->
  <link rel="icon" type="image/png" href="favicon.png" />

  <title>fonnx_example</title>
  <link rel="manifest" href="manifest.json">

  <script>
    // The value below is injected by flutter build, do not touch.
    const serviceWorkerVersion = null;
  </script>
  <!-- This script adds the flutter initialization JS code -->
  <script src="flutter.js" defer></script>
</head>

<body>
  <script>
    window.addEventListener('load', function (ev) {
      // Download main.dart.js
      _flutter.loader.loadEntrypoint({
        serviceWorker: {
          serviceWorkerVersion: serviceWorkerVersion,
        },
        onEntrypointLoaded: function (engineInitializer) {
          engineInitializer.initializeEngine().then(function (appRunner) {
            appRunner.runApp();
          });
        }
      });
    });
  </script>
  <!-- ONNX implementations start here
       Patterned after https://github.com/microsoft/onnxruntime-inference-examples/blob/main/js/quick-start_onnxruntime-web-script-tag/index.html
  -->
  <!-- REQUIRED FOR: ALL models. -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <!-- REQUIRED FOR: MiniLM models. -->
  <script>
    // Get the number of logical processors available.
    const cores = navigator.hardwareConcurrency;

    // Ensure at least 1 and at most half the number of hardwareConcurrency.
    // Testing showed using all cores was 10% slower than using half.
    // Tested on MBA M2 with a natural value of 8 for navigator.hardwareConcurrency.
    ort.env.wasm.numThreads = Math.max(1, Math.min(Math.floor(cores / 2), cores));
    let cachedModelPath = null;
    let modelPromise = null;

    const worker = new Worker('worker.js');

    // Simplified logs for brevity; can be extended to log each property if required.
    worker.onmessage = function (e) {
      const { messageId, action, embeddings, error } = e.data;
      if (action === "inferenceResult" && messageIdToResolve.has(messageId)) {
        messageIdToResolve.get(messageId)(embeddings);
        cleanup(messageId);
      } else if (action === "error" && messageIdToReject.has(messageId)) {
        messageIdToReject.get(messageId)(new Error(error));
        cleanup(messageId);
      }
    };

    const messageIdToResolve = new Map();
    const messageIdToReject = new Map();

    function cleanup(messageId) {
      messageIdToResolve.delete(messageId);
      messageIdToReject.delete(messageId);
    }

    function miniLmL6V2(modelPath, wordpieces) {
      return new Promise((resolve, reject) => {
        const messageId = Math.random().toString(36).substring(2);

        messageIdToResolve.set(messageId, resolve);
        messageIdToReject.set(messageId, reject);

        // If model path has changed or model is not yet loaded, fetch and load the model.
        if (cachedModelPath !== modelPath || !modelPromise) {
          cachedModelPath = modelPath;
          modelPromise = fetch(modelPath)
            .then(response => response.arrayBuffer())
            .then(modelArrayBuffer => {
              return new Promise((resolveLoad) => {
                // Post the load model message to the worker.
                worker.postMessage({
                  action: 'loadModel',
                  modelArrayBuffer,
                  messageId
                }, [modelArrayBuffer]);

                // Setup a one-time message listener for the "modelLoaded" message.
                const onModelLoaded = (e) => {
                  if (e.data.action === 'modelLoaded' && e.data.messageId === messageId) {
                    worker.removeEventListener('message', onModelLoaded);
                    resolveLoad();
                  }
                };
                worker.addEventListener('message', onModelLoaded);
              });
            })
            .catch(reject);
        }

        modelPromise.then(() => {
          // Once the model is loaded, send the run inference message to the worker.
          worker.postMessage({
            action: 'runInference',
            wordpieces,
            messageId
          });
        }).catch(reject);
      });
    }
  </script>
</body>

</html>