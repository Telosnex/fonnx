<!DOCTYPE html>
<html>

<head>
  <base href="$FLUTTER_BASE_HREF">

  <meta charset="UTF-8">
  <meta content="IE=Edge" http-equiv="X-UA-Compatible">
  <meta name="description" content="Demonstrates how to use the fonnx plugin.">

  <!-- iOS meta tags & icons -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="apple-mobile-web-app-title" content="fonnx_example">
  <link rel="apple-touch-icon" href="icons/Icon-192.png">

  <!-- Favicon -->
  <link rel="icon" type="image/png" href="favicon.png" />

  <title>fonnx_example</title>
  <link rel="manifest" href="manifest.json">

  <script>
    // The value below is injected by flutter build, do not touch.
    const serviceWorkerVersion = null;
  </script>
  <!-- This script adds the flutter initialization JS code -->
  <script src="flutter.js" defer></script>
</head>

<body>
  <script>
    window.addEventListener('load', function (ev) {
      // Download main.dart.js
      _flutter.loader.loadEntrypoint({
        serviceWorker: {
          serviceWorkerVersion: serviceWorkerVersion,
        },
        onEntrypointLoaded: function (engineInitializer) {
          engineInitializer.initializeEngine().then(function (appRunner) {
            appRunner.runApp();
          });
        }
      });
    });
  </script>
  <!-- ONNX implementations start here
       Patterned after https://github.com/microsoft/onnxruntime-inference-examples/blob/main/js/quick-start_onnxruntime-web-script-tag/index.html
  -->
  <!-- REQUIRED FOR: ALL models. -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <!-- REQUIRED FOR: MiniLM models. -->
  <!-- This script: -->
  <!-- 1. Sets up a web worker for inference to avoid blocking UI. -->
  <!-- 2. Loads the model into an array buffer. -->
  <!-- 3. Sends the array buffer to the worker. (ONNX can't load from a file in a worker) -->
  <!-- 4. Sends inference calls to the worker and returns a Promise. -->
  <!-- 5. When inference is complete, the Promise is resolved. -->
  <script>
    // Get the number of logical processors available.
    const cores = navigator.hardwareConcurrency;

    // Ensure at least 1 and at most half the number of hardwareConcurrency.
    // Testing showed using all cores was 10% slower than using half.
    // Tested on MBA M2 with a natural value of 8 for navigator.hardwareConcurrency.
    ort.env.wasm.numThreads = Math.max(1, Math.min(Math.floor(cores / 2), cores));
    let miniLmCachedModelPath = null;
    let miniLmModelPromise = null;

    const workerMiniLm = new Worker('worker_minilm.js');

    // Simplified logs for brevity; can be extended to log each property if required.
    workerMiniLm.onmessage = function (e) {
      const { messageId, action, embeddings, error } = e.data;
      if (action === "inferenceResult" && miniLmMessageIdToResolve.has(messageId)) {
        miniLmMessageIdToResolve.get(messageId)(embeddings);
        cleanup(messageId);
      } else if (action === "error" && miniLmMessageIdToReject.has(messageId)) {
        miniLmMessageIdToReject.get(messageId)(new Error(error));
        cleanup(messageId);
      }
    };

    const miniLmMessageIdToResolve = new Map();
    const miniLmMessageIdToReject = new Map();

    function cleanup(messageId) {
      miniLmMessageIdToResolve.delete(messageId);
      miniLmMessageIdToReject.delete(messageId);
    }

    function miniLmL6V2(modelPath, wordpieces) {
      return new Promise((resolve, reject) => {
        const messageId = Math.random().toString(36).substring(2);

        miniLmMessageIdToResolve.set(messageId, resolve);
        miniLmMessageIdToReject.set(messageId, reject);

        // If model path has changed or model is not yet loaded, fetch and load the model.
        if (miniLmCachedModelPath !== modelPath || !miniLmModelPromise) {
          miniLmCachedModelPath = modelPath;
          miniLmModelPromise = fetch(modelPath)
            .then(response => response.arrayBuffer())
            .then(modelArrayBuffer => {
              return new Promise((resolveLoad) => {
                // Post the load model message to the worker.
                workerMiniLm.postMessage({
                  action: 'loadModel',
                  modelArrayBuffer,
                  messageId
                }, [modelArrayBuffer]);

                // Setup a one-time message listener for the "modelLoaded" message.
                const onModelLoaded = (e) => {
                  if (e.data.action === 'modelLoaded' && e.data.messageId === messageId) {
                    workerMiniLm.removeEventListener('message', onModelLoaded);
                    resolveLoad();
                  }
                };
                workerMiniLm.addEventListener('message', onModelLoaded);
              });
            })
            .catch(reject);
        }

        miniLmModelPromise.then(() => {
          // Once the model is loaded, send the run inference message to the worker.
          workerMiniLm.postMessage({
            action: 'runInference',
            wordpieces,
            messageId
          });
        }).catch(reject);
      });
    }
  </script>
  <script>
    // Get the number of logical processors available.
    const whisperCores = navigator.hardwareConcurrency;

    // Ensure at least 1 and at most half the number of hardwareConcurrency.
    // Testing showed using all cores was 10% slower than using half.
    // Tested on MBA M2 with a natural value of 8 for navigator.hardwareConcurrency.
    ort.env.wasm.numThreads = Math.max(1, Math.min(Math.floor(whisperCores / 2), whisperCores));
    let whisperCachedModelPath = null;
    let whisperModelPromise = null;

    const workerWhisper = new Worker('worker_whisper.js');

    // Simplified logs for brevity; can be extended to log each property if required.
    workerWhisper.onmessage = function (e) {
      const { messageId, action, embeddings, error } = e.data;
      if (action === "inferenceResult" && whisperMessageIdToResolve.has(messageId)) {
        whisperMessageIdToResolve.get(messageId)(embeddings);
        cleanup(messageId);
      } else if (action === "error" && whisperMessageIdToReject.has(messageId)) {
        whisperMessageIdToReject.get(messageId)(new Error(error));
        cleanup(messageId);
      }
    };

    const whisperMessageIdToResolve = new Map();
    const whisperMessageIdToReject = new Map();

    function cleanup(messageId) {
      whisperMessageIdToResolve.delete(messageId);
      whisperMessageIdToReject.delete(messageId);
    }

    function whisper(modelPath, wordpieces) {
      return new Promise((resolve, reject) => {
        const messageId = Math.random().toString(36).substring(2);

        whisperMessageIdToResolve.set(messageId, resolve);
        whisperMessageIdToReject.set(messageId, reject);

        // If model path has changed or model is not yet loaded, fetch and load the model.
        if (whisperCachedModelPath !== modelPath || !whisperModelPromise) {
          whisperCachedModelPath = modelPath;
          whisperModelPromise = fetch(modelPath)
            .then(response => response.arrayBuffer())
            .then(modelArrayBuffer => {
              return new Promise((resolveLoad) => {
                // Post the load model message to the worker.
                workerWhisper.postMessage({
                  action: 'loadModel',
                  modelArrayBuffer,
                  messageId
                }, [modelArrayBuffer]);

                // Setup a one-time message listener for the "modelLoaded" message.
                const onModelLoaded = (e) => {
                  if (e.data.action === 'modelLoaded' && e.data.messageId === messageId) {
                    workerWhisper.removeEventListener('message', onModelLoaded);
                    resolveLoad();
                  }
                };
                workerWhisper.addEventListener('message', onModelLoaded);
              });
            })
            .catch(reject);
        }

        whisperModelPromise.then(() => {
          // Once the model is loaded, send the run inference message to the worker.
          workerWhisper.postMessage({
            action: 'runInference',
            wordpieces,
            messageId
          });
        }).catch(reject);
      });
    }
  </script>
</body>

</html>